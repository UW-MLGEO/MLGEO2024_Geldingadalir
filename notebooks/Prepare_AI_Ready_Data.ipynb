{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for the model used in the paper ##\n",
    "\n",
    "Read each file and segment them into one hour windows. Each file will contain 28,800 samples per hour at an 8 Hz sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from obspy import read\n",
    "import glob\n",
    "import numpy as np\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "# Define the processed and output folder paths\n",
    "processed_folder = os.getcwd() + '/data/processed'\n",
    "segmented_output_folder = os.getcwd() + '/data/segmented'\n",
    "os.makedirs(segmented_output_folder, exist_ok=True)  # Create output directory if it doesn't exist\n",
    "\n",
    "# Define the target segment duration (1 hour) in seconds\n",
    "segment_duration = 3600  # seconds\n",
    "\n",
    "# Process each processed file for segmentation\n",
    "for file_path in glob.glob(f\"{processed_folder}/*.mseed\"):\n",
    "    # Read the processed file\n",
    "    st = read(file_path)\n",
    "    \n",
    "    # Normalize the traces in the stream object - done in data_cleaning\n",
    "    #st.normalize()\n",
    "    \n",
    "    # Split each trace in the Stream object into one-hour segments\n",
    "    for tr in st:\n",
    "        start_time = tr.stats.starttime\n",
    "        end_time = tr.stats.endtime\n",
    "        segment_start = start_time\n",
    "        \n",
    "        # Loop over each one-hour segment\n",
    "        while segment_start + segment_duration <= end_time:\n",
    "            # Define the end time for the current segment\n",
    "            segment_end = segment_start + segment_duration\n",
    "\n",
    "            # Slice the trace to create a one-hour segment\n",
    "            segment = tr.slice(starttime=segment_start, endtime=segment_end)\n",
    "            \n",
    "            # Format the filename for the segment\n",
    "            segment_filename = f\"{tr.stats.network}_{tr.stats.station}_{tr.stats.channel}_{segment_start.strftime('%Y%m%dT%H%M%S')}.mseed\"\n",
    "            segment_filepath = os.path.join(segmented_output_folder, segment_filename)\n",
    "            \n",
    "            # Save the one-hour segment as a new file\n",
    "            try:\n",
    "                segment.write(segment_filepath, format=\"MSEED\")\n",
    "                print(f\"Saved segment file: {segment_filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing segment {segment_filename}: {e}\")\n",
    "\n",
    "            # Move to the next hour\n",
    "            segment_start = segment_end\n",
    "\n",
    "            # # Save the one-hour segment as a new file\n",
    "            # segment.write(segment_filepath, format=\"MSEED\")\n",
    "            # print(f'Saved segment file: {segment_filepath}')\n",
    "\n",
    "            # # Move to the next hour\n",
    "            # segment_start = segment_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all files in the segmented folder for nan data\n",
    "\n",
    "for file_path in glob.glob(f\"{segmented_output_folder}/*.mseed\"):\n",
    "    st = read(file_path)\n",
    "    for tr in st:\n",
    "        if np.isnan(tr.data).any():\n",
    "            print(f\"File {file_path} contains NaN data\")\n",
    "        # else:\n",
    "        #     print(f\"File {file_path} is clean\")\n",
    "print(f'done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the Fourier Transform with the `specified window_length` and `hop_length parameters` to produce a spectrogram of size (96, 128) for each one-hour segment. The data is saved in `data\\segmented` from the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from obspy import read\n",
    "import librosa\n",
    "from scipy.signal import resample\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "import torch\n",
    "\n",
    "# Define directories\n",
    "input_folder = os.getcwd() + '/data/raw'\n",
    "processed_folder = os.getcwd() + '/data/processed'\n",
    "segmented_output_folder = os.getcwd() + '/data/segmented'\n",
    "os.makedirs(segmented_output_folder, exist_ok=True)  # Create output directory if it doesn't exist\n",
    "\n",
    "# Define constants\n",
    "target_sampling_rate = 8  # Hz\n",
    "segment_duration = 3600  # 1 hour in seconds\n",
    "window_length = 256\n",
    "hop_length = 224\n",
    "target_shape = (96, 128)\n",
    "\n",
    "# Initialize an empty list to store spectrograms\n",
    "spectrogram_list = []\n",
    "\n",
    "# Step 1: Process each file in the input folder\n",
    "for file_path in glob.glob(f\"{input_folder}/*.mseed\"):\n",
    "    # Read the file\n",
    "    st = read(file_path)\n",
    "    \n",
    "    # Process each trace\n",
    "    for tr in st:\n",
    "        # Resample the data to 8 Hz (assuming original sampling rate was 100 Hz)\n",
    "        # this is done in data cleaning\n",
    "        #tr.resample(target_sampling_rate)\n",
    "        \n",
    "        # Split into one-hour segments (28,800 samples each)\n",
    "        start_time = tr.stats.starttime\n",
    "        end_time = tr.stats.endtime\n",
    "        segment_start = start_time\n",
    "\n",
    "        while segment_start + segment_duration <= end_time:\n",
    "            # Define the end time for the current segment\n",
    "            segment_end = segment_start + segment_duration\n",
    "\n",
    "            # Slice the trace to create a one-hour segment\n",
    "            segment = tr.slice(starttime=segment_start, endtime=segment_end)\n",
    "            \n",
    "            \"\"\"\n",
    "            # Compute STFT for the segment\n",
    "            signal = segment.data.astype(np.float32)\n",
    "            stft_result = librosa.stft(signal, n_fft=window_length, hop_length=hop_length, win_length=window_length)\n",
    "            \n",
    "            # use obspy stft\n",
    "            #stft_result = segment.spectrogram(wlen=window_length, per_lap=0.9, log=False)\n",
    "            spectrogram = np.abs(stft_result) ** 2\n",
    "\n",
    "            # Resize spectrogram to target shape\n",
    "            spectrogram_resized = librosa.util.fix_length(spectrogram, size=target_shape[1], axis=1)[:target_shape[0], :]\n",
    "            \n",
    "            # Apply log transformation first\n",
    "            spectrogram_resized = np.maximum(spectrogram_resized, 1e-12)\n",
    "            spectrogram_log = np.log1p(spectrogram_resized)\n",
    "\n",
    "            # Normalize the log-transformed data between -1 and 1\n",
    "            spectrogram_log = 2 * (spectrogram_log - np.min(spectrogram_log)) / np.ptp(spectrogram_log) - 1\n",
    "            \n",
    "            \"\"\"\n",
    "            ## NEW ##\n",
    "            # Convert to PyTorch tensor\n",
    "            segment_tensor = torch.tensor(segment, dtype=torch.float32)\n",
    "            \n",
    "            # Ensure correct dimensions (STFT expects 1D tensor or batch x time)\n",
    "            if len(segment_tensor.shape) == 1:\n",
    "                segment_tensor = segment_tensor.unsqueeze(0)\n",
    "                \n",
    "            # Compute the STFT\n",
    "            stft_result = torch.stft(\n",
    "                segment_tensor,\n",
    "                n_fft=window_length,\n",
    "                hop_length=hop_length,\n",
    "                win_length=window_length,\n",
    "                return_complex=True,\n",
    "            )\n",
    "                \n",
    "            # Compute the spectrogram (magnitude squared)\n",
    "            spectrogram = torch.abs(stft_result) ** 2\n",
    "\n",
    "            # Convert to NumPy for further processing\n",
    "            spectrogram = spectrogram.numpy()\n",
    "\n",
    "            # Resize to a consistent shape if needed (example using interpolation)\n",
    "            target_shape = (96, 128)  # Desired shape\n",
    "            spectrogram_resized = np.resize(spectrogram, target_shape)\n",
    "\n",
    "            # Log-transform the spectrogram\n",
    "            spectrogram_log = np.log1p(spectrogram_resized)\n",
    "\n",
    "            # Normalize between -1 and 1\n",
    "            spectrogram_log = 2 * (spectrogram_log - np.min(spectrogram_log)) / np.ptp(spectrogram_log) - 1\n",
    "                    ## END NEW ##\n",
    "\n",
    "            # Save the spectrogram (you could save the result as needed)\n",
    "            segment_filename = f\"{tr.stats.network}_{tr.stats.station}_{tr.stats.channel}_{segment_start.strftime('%Y%m%dT%H%M%S')}.npy\"\n",
    "            segment_filepath = os.path.join(segmented_output_folder, segment_filename)\n",
    "\n",
    "            np.save(segment_filepath, spectrogram_log)\n",
    "\n",
    "            # Append the spectrogram to the list\n",
    "            spectrogram_list.append(spectrogram_log)\n",
    "            \n",
    "            # check the file for data outside the bounds of -1 and 1\n",
    "            if np.any(spectrogram_log > 1) or np.any(spectrogram_log < -1):\n",
    "                print(f\"File {segment_filename} contains data outside the bounds of -1 and 1\")\n",
    "\n",
    "            # Move to the next hour\n",
    "            segment_start = segment_end\n",
    "            \n",
    "            # display file being processed\n",
    "            # time.sleep(0.1)\n",
    "            # clear_output(wait=True)\n",
    "            print(f'Processing file: {segment_filename}')\n",
    "            \n",
    "            #check file for nan data\n",
    "            if np.isnan(spectrogram_log).any():\n",
    "                print(f\"File {segment_filename} contains NaN data\")             \n",
    "\n",
    "# Stack all spectrograms into a single numpy array and add batch and channel dimensions\n",
    "all_spectrograms = np.stack(spectrogram_list)\n",
    "all_spectrograms = all_spectrograms[:, np.newaxis, :, :]  # Shape: (batch_size, 1, 96, 128)\n",
    "print(f\"Shape of combined spectrogram array: {all_spectrograms.shape}\")\n",
    "\n",
    "# remove the single feature dimension (1)\n",
    "all_spectrograms = np.squeeze(all_spectrograms)\n",
    "\n",
    "# NOTE: FILE NAME IS HARD CODED HERE\n",
    "# Save the combined spectrogram array to Input.npy\n",
    "final_output_folder = os.getcwd()\n",
    "input_filepath = os.path.join(final_output_folder, \"Input.npy\")\n",
    "#input_filepath = os.path.join(final_output_folder + 'NUPH_analysis', \"Input_nuph.npy\")\n",
    "np.save(input_filepath, all_spectrograms)\n",
    "\n",
    "print(f\"Saved combined spectrogram file: {input_filepath.split('/')[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Ensure there are spectrograms to plot\n",
    "# if all_spectrograms.size > 0:\n",
    "#     length = len(all_spectrograms)\n",
    "#     print(length)\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     for i in range(length):  # Loop through all spectrograms\n",
    "#         plt.plot(all_spectrograms[i].flatten())\n",
    "#     plt.title(f'Spectrograms as Line Plot')\n",
    "#     plt.xlabel('Frequency Bins')\n",
    "#     plt.ylabel('Log Power')\n",
    "#     plt.show()\n",
    "# else:\n",
    "#     print(\"No spectrograms found in all_spectrograms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display information on Input.npy\n",
    "print(f\"Shape of combined spectrogram array with batch and channel dimensions: {all_spectrograms.shape}\")\n",
    "print(f\"Number of files processed: {all_spectrograms.shape[0]}\")\n",
    "\n",
    "# load Input.npy and display information for confirmation\n",
    "input_data_file_t = np.load(input_filepath)\n",
    "print(f\"Shape of combined spectrogram array Input.npy: {input_data_file_t.shape}\")\n",
    "print(f\"Number of files processed: {input_data_file_t.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data in all_spectograms for any nans\n",
    "nan_check = np.isnan(all_spectrograms)\n",
    "nan_count = np.sum(nan_check)\n",
    "print(f\"Number of NaNs in all_spectrograms: {nan_count}\")\n",
    "\n",
    "# how much data is NOT nan?\n",
    "nan_count = np.sum(~nan_check)\n",
    "print(f\"Number of non-NaNs in all_spectrograms: {nan_count}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Input shape is [all_spectrograms.shape[0], 1, 96, 128] where:\n",
    "\n",
    "all_spectrograms.shape[0]: batch size -> the number of hours of data processed\n",
    "1: input channel. Probably indicates there is only one feature per time step\n",
    "96: height (or time)\n",
    "128: width (or frequency)  \n",
    "\n",
    "11/14/24 - input channel removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first five rows in the all_spectograms\n",
    "print(all_spectrograms[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
