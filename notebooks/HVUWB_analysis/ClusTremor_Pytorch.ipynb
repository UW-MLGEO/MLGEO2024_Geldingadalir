{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855ba09c",
   "metadata": {},
   "source": [
    "# Set up your environment\n",
    "\n",
    "Use the mlgeo enviroment from class\n",
    "Install any missing packages\n",
    "TODO: create new env file that includes all of these packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d453252-815f-4065-8315-7a65b23d3b65",
   "metadata": {},
   "source": [
    "## initial installation\n",
    "### start with environment from class\n",
    "\n",
    "%pip install h5py\n",
    "%pip install keras\n",
    "%pip install --upgrade tensorflow\n",
    "%pip install scikit-learn\n",
    "%pip install seaborn\n",
    "%pip install pydot\n",
    "%pip install pydot graphviz     # for plotting model\n",
    "%pip install librosa\n",
    "%pip show tensorflow\n",
    "\n",
    "#TODO write requirements.txt to include all of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd2b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os, re, glob\n",
    "import math\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import savefig\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Dropout, Reshape \n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization, ZeroPadding1D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, Adam, schedules\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef79091",
   "metadata": {},
   "source": [
    "# Load data and split train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f5e562e-458d-4f4d-98ad-afc1778647c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning to load file...\n",
      "File loaded\n",
      "Split data into training and testing sets\n",
      "Size of data_file: (1560, 96, 128)\n",
      "Size of train set: (1248, 96, 128)\n",
      "Size of test set: (312, 96, 128)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Step 1: Load data\n",
    "print(\"Beginning to load file...\")\n",
    "\n",
    "# Load the seismic data (adjust file path as needed)\n",
    "data_file = np.load(\"Input.npy\", mmap_mode='r', allow_pickle=True)\n",
    "\n",
    "print(\"File loaded\")\n",
    "\n",
    "# Step 2: Split data into train and test sets\n",
    "print(\"Split data into training and testing sets\")\n",
    "train_data, test_data = train_test_split(data_file, test_size=0.2, random_state=46, shuffle=True)\n",
    "\n",
    "print('Size of data_file:', data_file.shape)\n",
    "print('Size of train set:', train_data.shape)\n",
    "print('Size of test set:', test_data.shape)\n",
    "\n",
    "# Step 3: Convert the data into PyTorch tensors\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32)\n",
    "\n",
    "# If the data has more dimensions, you can reshape or modify it as needed, but\n",
    "# this will depend on the shape of your `Input.npy` file.\n",
    "\n",
    "# Example (assuming 4D input with channels, height, and width):\n",
    "# train_data_tensor = train_data_tensor.view(-1, 1, height, width)\n",
    "# test_data_tensor = test_data_tensor.view(-1, 1, height, width)\n",
    "\n",
    "# Step 4: Prepare DataLoader for batching\n",
    "batch_size = 32\n",
    "\n",
    "# Create TensorDataset and DataLoader for training and testing sets\n",
    "train_dataset = TensorDataset(train_data_tensor, train_data_tensor)  # (input, target) both are the same for autoencoder\n",
    "test_dataset = TensorDataset(test_data_tensor, test_data_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Data is now ready for training in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e4850-cc71-4419-a0f0-641ba43f765a",
   "metadata": {},
   "source": [
    "# Autoencoder architecture construction\n",
    "\n",
    "Import the dependencies first!\n",
    "!pip install torch\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f58b71a-84ff-417c-8261-54b91ce52507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Autoencoder                              [1, 1, 96, 128]           --\n",
       "├─Conv2d: 1-1                            [1, 8, 48, 64]            288\n",
       "├─Conv2d: 1-2                            [1, 16, 24, 32]           1,936\n",
       "├─Conv2d: 1-3                            [1, 32, 12, 16]           7,712\n",
       "├─Conv2d: 1-4                            [1, 64, 6, 8]             30,784\n",
       "├─Flatten: 1-5                           [1, 3072]                 --\n",
       "├─Linear: 1-6                            [1, 24]                   73,752\n",
       "├─Linear: 1-7                            [1, 3072]                 76,800\n",
       "├─Linear: 1-8                            [1, 3072]                 9,440,256\n",
       "├─ConvTranspose2d: 1-9                   [1, 32, 12, 16]           30,752\n",
       "├─ConvTranspose2d: 1-10                  [1, 16, 24, 32]           7,696\n",
       "├─ConvTranspose2d: 1-11                  [1, 8, 48, 64]            1,928\n",
       "├─ConvTranspose2d: 1-12                  [1, 1, 96, 128]           281\n",
       "==========================================================================================\n",
       "Total params: 9,672,185\n",
       "Trainable params: 9,672,185\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 36.11\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.86\n",
       "Params size (MB): 38.69\n",
       "Estimated Total Size (MB): 39.60\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "sd = 46\n",
    "torch.manual_seed(sd)\n",
    "\n",
    "# Function to calculate \"same\" padding for strided convolutions\n",
    "def calculate_same_padding(kernel_size, stride):\n",
    "    return ((stride - 1) + (kernel_size - 1)) // 2\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder layers with manually calculated \"same\" padding\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(7, 5), stride=2, padding=(calculate_same_padding(7, 2), calculate_same_padding(5, 2)))\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(5, 3), stride=2, padding=(calculate_same_padding(5, 2), calculate_same_padding(3, 2)))\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(5, 3), stride=2, padding=(calculate_same_padding(5, 2), calculate_same_padding(3, 2)))\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(5, 3), stride=2, padding=(calculate_same_padding(5, 2), calculate_same_padding(3, 2)))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 6 * 8, 24)  # Adjusted based on conv output\n",
    "        self.fc2 = nn.Linear(24, 3072)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(3072, 64 * 6 * 8)  # Adjust based on the shape before flattening\n",
    "        self.conv_transpose1 = nn.ConvTranspose2d(64, 32, kernel_size=(5, 3), stride=2, padding=(2, 1), output_padding=(1, 1))  # Adjust output_padding\n",
    "        self.conv_transpose2 = nn.ConvTranspose2d(32, 16, kernel_size=(5, 3), stride=2, padding=(2, 1), output_padding=(1, 1))\n",
    "        self.conv_transpose3 = nn.ConvTranspose2d(16, 8, kernel_size=(5, 3), stride=2, padding=(2, 1), output_padding=(1, 1))\n",
    "        self.conv_transpose4 = nn.ConvTranspose2d(8, 1, kernel_size=(7, 5), stride=2, padding=(3, 2), output_padding=(1, 1))  # Adjust padding/output_padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "\n",
    "        # Save shape for later use in decoder\n",
    "        shape_before_flattening = x.shape\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.flatten(x)\n",
    "        encoded = F.elu(self.fc1(x))\n",
    "        x = F.elu(self.fc2(encoded))\n",
    "\n",
    "        # Decoder\n",
    "        x = F.elu(self.fc3(x))\n",
    "        x = x.view(-1, 64, 6, 8)  # Reshape to the shape before flattening\n",
    "        x = F.elu(self.conv_transpose1(x))\n",
    "        x = F.elu(self.conv_transpose2(x))\n",
    "        x = F.elu(self.conv_transpose3(x))\n",
    "        decoded = self.conv_transpose4(x)  # No activation for output layer\n",
    "\n",
    "        return decoded, encoded\n",
    "\n",
    "\n",
    "# Create model instance\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device\n",
    "autoencoder.to(device)\n",
    "\n",
    "# Ensure that the input data is on the same device (for example, random input)\n",
    "dummy_input = torch.randn(1, 1, 96, 128).to(device)  # Simulate a batch of size 1\n",
    "\n",
    "# Now use torchinfo for the summary\n",
    "summary(autoencoder, input_size=(1, 1, 96, 128))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ebb90-94ed-4ffd-8627-3ca2b0deff1b",
   "metadata": {},
   "source": [
    "# Initial training phase: Pretraining the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6564b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Train Loss: 701.5801, Val Loss: 213.0833, LR: 0.001000\n",
      "Epoch 2/500, Train Loss: 64.4923, Val Loss: 23.9677, LR: 0.000999\n",
      "Epoch 3/500, Train Loss: 21.0478, Val Loss: 18.7399, LR: 0.000999\n",
      "Epoch 4/500, Train Loss: 17.8624, Val Loss: 16.7897, LR: 0.000998\n",
      "Epoch 5/500, Train Loss: 16.5532, Val Loss: 15.2239, LR: 0.000997\n",
      "Epoch 6/500, Train Loss: 14.5770, Val Loss: 14.2374, LR: 0.000997\n",
      "Epoch 7/500, Train Loss: 13.4889, Val Loss: 13.0389, LR: 0.000996\n",
      "Epoch 8/500, Train Loss: 12.6564, Val Loss: 12.1943, LR: 0.000995\n",
      "Epoch 9/500, Train Loss: 11.8596, Val Loss: 11.5182, LR: 0.000994\n",
      "Epoch 10/500, Train Loss: 11.0853, Val Loss: 10.8836, LR: 0.000994\n",
      "Epoch 11/500, Train Loss: 10.7041, Val Loss: 10.4980, LR: 0.000993\n",
      "Epoch 12/500, Train Loss: 10.1534, Val Loss: 10.8191, LR: 0.000992\n",
      "Epoch 13/500, Train Loss: 9.7798, Val Loss: 9.5125, LR: 0.000992\n",
      "Epoch 14/500, Train Loss: 9.2885, Val Loss: 8.9681, LR: 0.000991\n",
      "Epoch 15/500, Train Loss: 8.8645, Val Loss: 8.6616, LR: 0.000990\n",
      "Epoch 16/500, Train Loss: 8.5170, Val Loss: 8.3093, LR: 0.000990\n",
      "Epoch 17/500, Train Loss: 8.2210, Val Loss: 8.0869, LR: 0.000989\n",
      "Epoch 18/500, Train Loss: 8.1790, Val Loss: 7.6479, LR: 0.000988\n",
      "Epoch 19/500, Train Loss: 7.8811, Val Loss: 7.8138, LR: 0.000988\n",
      "Epoch 20/500, Train Loss: 7.3074, Val Loss: 7.1451, LR: 0.000987\n",
      "Epoch 21/500, Train Loss: 6.9909, Val Loss: 6.8243, LR: 0.000986\n",
      "Epoch 22/500, Train Loss: 6.7069, Val Loss: 6.5493, LR: 0.000986\n",
      "Epoch 23/500, Train Loss: 6.4419, Val Loss: 6.4227, LR: 0.000985\n",
      "Epoch 24/500, Train Loss: 6.2957, Val Loss: 6.0551, LR: 0.000984\n",
      "Epoch 25/500, Train Loss: 5.9699, Val Loss: 5.8167, LR: 0.000984\n",
      "Epoch 26/500, Train Loss: 5.8012, Val Loss: 5.5509, LR: 0.000983\n",
      "Epoch 27/500, Train Loss: 5.7112, Val Loss: 5.3618, LR: 0.000982\n",
      "Epoch 28/500, Train Loss: 5.5098, Val Loss: 5.1299, LR: 0.000981\n",
      "Epoch 29/500, Train Loss: 5.0557, Val Loss: 5.2212, LR: 0.000981\n",
      "Epoch 30/500, Train Loss: 4.9088, Val Loss: 4.6499, LR: 0.000980\n",
      "Epoch 31/500, Train Loss: 4.7297, Val Loss: 4.4582, LR: 0.000979\n",
      "Epoch 32/500, Train Loss: 4.4811, Val Loss: 4.6846, LR: 0.000979\n",
      "Epoch 33/500, Train Loss: 4.3015, Val Loss: 4.1552, LR: 0.000978\n",
      "Epoch 34/500, Train Loss: 4.1328, Val Loss: 3.9582, LR: 0.000977\n",
      "Epoch 35/500, Train Loss: 3.9786, Val Loss: 3.9151, LR: 0.000977\n",
      "Epoch 36/500, Train Loss: 3.9259, Val Loss: 3.9141, LR: 0.000976\n",
      "Epoch 37/500, Train Loss: 3.9013, Val Loss: 3.6086, LR: 0.000975\n",
      "Epoch 38/500, Train Loss: 3.6379, Val Loss: 3.5640, LR: 0.000975\n",
      "Epoch 39/500, Train Loss: 3.7014, Val Loss: 3.4654, LR: 0.000974\n",
      "Epoch 40/500, Train Loss: 3.5085, Val Loss: 3.5935, LR: 0.000973\n",
      "Epoch 41/500, Train Loss: 3.5781, Val Loss: 3.3406, LR: 0.000973\n",
      "Epoch 42/500, Train Loss: 3.4051, Val Loss: 3.3039, LR: 0.000972\n",
      "Epoch 43/500, Train Loss: 3.3054, Val Loss: 3.1986, LR: 0.000971\n",
      "Epoch 44/500, Train Loss: 3.2138, Val Loss: 3.1573, LR: 0.000971\n",
      "Epoch 45/500, Train Loss: 3.3283, Val Loss: 3.1624, LR: 0.000970\n",
      "Epoch 46/500, Train Loss: 3.4283, Val Loss: 3.2455, LR: 0.000969\n",
      "Epoch 47/500, Train Loss: 3.1415, Val Loss: 3.1772, LR: 0.000969\n",
      "Epoch 48/500, Train Loss: 3.0799, Val Loss: 3.0239, LR: 0.000968\n",
      "Epoch 49/500, Train Loss: 3.0499, Val Loss: 3.1602, LR: 0.000967\n",
      "Epoch 50/500, Train Loss: 3.0710, Val Loss: 2.9746, LR: 0.000967\n",
      "Epoch 51/500, Train Loss: 3.0873, Val Loss: 3.1291, LR: 0.000966\n",
      "Epoch 52/500, Train Loss: 2.9659, Val Loss: 3.0151, LR: 0.000965\n",
      "Epoch 53/500, Train Loss: 2.9721, Val Loss: 2.9009, LR: 0.000965\n",
      "Epoch 54/500, Train Loss: 2.8812, Val Loss: 2.8338, LR: 0.000964\n",
      "Epoch 55/500, Train Loss: 2.9249, Val Loss: 2.8419, LR: 0.000963\n",
      "Epoch 56/500, Train Loss: 2.9851, Val Loss: 2.9707, LR: 0.000963\n",
      "Epoch 57/500, Train Loss: 2.8715, Val Loss: 2.8681, LR: 0.000962\n",
      "Epoch 58/500, Train Loss: 2.9178, Val Loss: 2.9001, LR: 0.000961\n",
      "Epoch 59/500, Train Loss: 2.9011, Val Loss: 2.7407, LR: 0.000961\n",
      "Epoch 60/500, Train Loss: 2.9198, Val Loss: 2.7527, LR: 0.000960\n",
      "Epoch 61/500, Train Loss: 2.8217, Val Loss: 2.9226, LR: 0.000959\n",
      "Epoch 62/500, Train Loss: 2.7707, Val Loss: 2.7103, LR: 0.000959\n",
      "Epoch 63/500, Train Loss: 2.7341, Val Loss: 2.6891, LR: 0.000958\n",
      "Epoch 64/500, Train Loss: 2.8299, Val Loss: 2.7269, LR: 0.000957\n",
      "Epoch 65/500, Train Loss: 2.9300, Val Loss: 3.2445, LR: 0.000957\n",
      "Epoch 66/500, Train Loss: 2.9130, Val Loss: 2.6739, LR: 0.000956\n",
      "Epoch 67/500, Train Loss: 2.7280, Val Loss: 2.9082, LR: 0.000955\n",
      "Epoch 68/500, Train Loss: 2.7047, Val Loss: 2.6460, LR: 0.000955\n",
      "Epoch 69/500, Train Loss: 2.6616, Val Loss: 2.6157, LR: 0.000954\n",
      "Epoch 70/500, Train Loss: 2.6647, Val Loss: 2.5973, LR: 0.000953\n",
      "Epoch 71/500, Train Loss: 2.7395, Val Loss: 2.8719, LR: 0.000953\n",
      "Epoch 72/500, Train Loss: 2.7848, Val Loss: 2.5796, LR: 0.000952\n",
      "Epoch 73/500, Train Loss: 2.6465, Val Loss: 2.7030, LR: 0.000951\n",
      "Epoch 74/500, Train Loss: 2.6159, Val Loss: 2.6451, LR: 0.000951\n",
      "Epoch 75/500, Train Loss: 2.6728, Val Loss: 2.5726, LR: 0.000950\n",
      "Epoch 76/500, Train Loss: 2.6289, Val Loss: 2.5912, LR: 0.000949\n",
      "Epoch 77/500, Train Loss: 2.6034, Val Loss: 2.5523, LR: 0.000949\n",
      "Epoch 78/500, Train Loss: 2.6382, Val Loss: 2.6702, LR: 0.000948\n",
      "Epoch 79/500, Train Loss: 2.6303, Val Loss: 2.6533, LR: 0.000947\n",
      "Epoch 80/500, Train Loss: 2.5919, Val Loss: 2.6153, LR: 0.000947\n",
      "Epoch 81/500, Train Loss: 2.6932, Val Loss: 3.5919, LR: 0.000946\n",
      "Epoch 82/500, Train Loss: 2.6738, Val Loss: 2.5779, LR: 0.000945\n",
      "Epoch 83/500, Train Loss: 2.6214, Val Loss: 2.5888, LR: 0.000945\n",
      "Epoch 84/500, Train Loss: 2.5352, Val Loss: 2.6492, LR: 0.000944\n",
      "Epoch 85/500, Train Loss: 2.6031, Val Loss: 2.5496, LR: 0.000943\n",
      "Epoch 86/500, Train Loss: 2.5840, Val Loss: 2.5959, LR: 0.000943\n",
      "Epoch 87/500, Train Loss: 2.6298, Val Loss: 2.7010, LR: 0.000942\n",
      "Epoch 88/500, Train Loss: 2.5638, Val Loss: 2.6019, LR: 0.000941\n",
      "Epoch 89/500, Train Loss: 2.5355, Val Loss: 2.5860, LR: 0.000941\n",
      "Epoch 90/500, Train Loss: 2.5309, Val Loss: 2.4692, LR: 0.000940\n",
      "Epoch 91/500, Train Loss: 2.5428, Val Loss: 2.7938, LR: 0.000940\n",
      "Epoch 92/500, Train Loss: 2.6686, Val Loss: 2.4742, LR: 0.000939\n",
      "Epoch 93/500, Train Loss: 2.5602, Val Loss: 2.4693, LR: 0.000938\n",
      "Epoch 94/500, Train Loss: 2.4919, Val Loss: 2.4807, LR: 0.000938\n",
      "Epoch 95/500, Train Loss: 2.4955, Val Loss: 2.6076, LR: 0.000937\n",
      "Epoch 96/500, Train Loss: 2.4931, Val Loss: 2.4349, LR: 0.000936\n",
      "Epoch 97/500, Train Loss: 2.4891, Val Loss: 2.5038, LR: 0.000936\n",
      "Epoch 98/500, Train Loss: 2.5431, Val Loss: 2.4991, LR: 0.000935\n",
      "Epoch 99/500, Train Loss: 2.6789, Val Loss: 2.7113, LR: 0.000934\n",
      "Epoch 100/500, Train Loss: 2.5434, Val Loss: 2.4628, LR: 0.000934\n",
      "Epoch 101/500, Train Loss: 2.4463, Val Loss: 2.4087, LR: 0.000933\n",
      "Epoch 102/500, Train Loss: 2.4622, Val Loss: 2.4266, LR: 0.000932\n",
      "Epoch 103/500, Train Loss: 2.6317, Val Loss: 2.9500, LR: 0.000932\n",
      "Epoch 104/500, Train Loss: 2.5853, Val Loss: 2.3986, LR: 0.000931\n",
      "Epoch 105/500, Train Loss: 2.4196, Val Loss: 2.4021, LR: 0.000930\n",
      "Epoch 106/500, Train Loss: 2.4148, Val Loss: 2.3887, LR: 0.000930\n",
      "Epoch 107/500, Train Loss: 2.4481, Val Loss: 2.4000, LR: 0.000929\n",
      "Epoch 108/500, Train Loss: 2.4723, Val Loss: 2.3786, LR: 0.000929\n",
      "Epoch 109/500, Train Loss: 2.4425, Val Loss: 2.6603, LR: 0.000928\n",
      "Epoch 110/500, Train Loss: 2.4926, Val Loss: 2.3848, LR: 0.000927\n",
      "Epoch 111/500, Train Loss: 2.4461, Val Loss: 2.3749, LR: 0.000927\n",
      "Epoch 112/500, Train Loss: 2.4334, Val Loss: 2.3975, LR: 0.000926\n",
      "Epoch 113/500, Train Loss: 2.4383, Val Loss: 2.3611, LR: 0.000925\n",
      "Epoch 114/500, Train Loss: 2.4116, Val Loss: 2.3902, LR: 0.000925\n",
      "Epoch 115/500, Train Loss: 2.4199, Val Loss: 2.4408, LR: 0.000924\n",
      "Epoch 116/500, Train Loss: 2.5133, Val Loss: 2.4422, LR: 0.000923\n",
      "Epoch 117/500, Train Loss: 2.3884, Val Loss: 2.3863, LR: 0.000923\n",
      "Epoch 118/500, Train Loss: 2.3689, Val Loss: 2.3482, LR: 0.000922\n",
      "Epoch 119/500, Train Loss: 2.4033, Val Loss: 2.3446, LR: 0.000921\n",
      "Epoch 120/500, Train Loss: 2.3780, Val Loss: 2.3722, LR: 0.000921\n",
      "Epoch 121/500, Train Loss: 2.3799, Val Loss: 2.4932, LR: 0.000920\n",
      "Epoch 122/500, Train Loss: 2.4201, Val Loss: 2.3336, LR: 0.000920\n",
      "Epoch 123/500, Train Loss: 2.4364, Val Loss: 2.3578, LR: 0.000919\n",
      "Epoch 124/500, Train Loss: 3.1101, Val Loss: 2.6361, LR: 0.000918\n",
      "Epoch 125/500, Train Loss: 2.8125, Val Loss: 2.5419, LR: 0.000918\n",
      "Epoch 126/500, Train Loss: 2.4129, Val Loss: 2.3453, LR: 0.000917\n",
      "Epoch 127/500, Train Loss: 2.3654, Val Loss: 2.4342, LR: 0.000916\n",
      "Epoch 128/500, Train Loss: 2.3966, Val Loss: 2.3227, LR: 0.000916\n",
      "Epoch 129/500, Train Loss: 2.3606, Val Loss: 2.3199, LR: 0.000915\n",
      "Epoch 130/500, Train Loss: 2.3321, Val Loss: 2.3205, LR: 0.000914\n",
      "Epoch 131/500, Train Loss: 2.3348, Val Loss: 2.3234, LR: 0.000914\n",
      "Epoch 132/500, Train Loss: 2.3432, Val Loss: 2.3160, LR: 0.000913\n",
      "Epoch 133/500, Train Loss: 2.3568, Val Loss: 2.3159, LR: 0.000913\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Define constants\n",
    "NUM_OF_EPOCHS = 500\n",
    "initial_lr = 0.001\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.5\n",
    "patience = 30\n",
    "\n",
    "# Initialize model, optimizer, and scheduler\n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "scheduler = ExponentialLR(optimizer, gamma=decay_rate**(1 / decay_steps))\n",
    "\n",
    "# Setup CSV logger\n",
    "csv_log = {'epoch': [], 'train_loss': [], 'val_loss': [], 'learning_rate': []}\n",
    "\n",
    "# Early stopping parameters\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Training loop with early stopping and logging\n",
    "for epoch in range(NUM_OF_EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    ####### DEBUGGING: Check input shape to model\n",
    "    # Check initial shape for debugging\n",
    "    # for data, _ in train_loader:\n",
    "    #     print(\"Initial data shape:\", data.shape)  # Uncomment to debug shape\n",
    "\n",
    "    #     # Ensure single channel\n",
    "    #     if data.dim() == 3:\n",
    "    #         data = data.unsqueeze(1)  # Add channel dimension if necessary\n",
    "\n",
    "    #     inputs = data.to(device)\n",
    "    #     print(\"Input shape to model:\", inputs.shape)  # Uncomment to verify final shape\n",
    "\n",
    "    #     optimizer.zero_grad()\n",
    "    #     outputs, _ = model(inputs)\n",
    "    #     loss = criterion(outputs, inputs)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    ####### END OF DEBUGGING\n",
    "    for data, _ in train_loader:\n",
    "        \n",
    "        data = data.squeeze().unsqueeze(1)  # Remove the extra dimension\n",
    "        inputs = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            \n",
    "            #data = data.unsqueeze(1)\n",
    "            data = data.squeeze().unsqueeze(1)  # Ensure correct shape\n",
    "            inputs = data.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Average losses\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(test_loader)\n",
    "\n",
    "    # Log values\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    csv_log['epoch'].append(epoch + 1)\n",
    "    csv_log['train_loss'].append(train_loss)\n",
    "    csv_log['val_loss'].append(val_loss)\n",
    "    csv_log['learning_rate'].append(current_lr)\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f'Val loss decreased from {best_val_loss:.4f} to {val_loss:.4f}')\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{NUM_OF_EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "# Save the CSV log\n",
    "pd.DataFrame(csv_log).to_csv('pretrain_log.csv', index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "pretraining_ac_run_time = end_time - start_time\n",
    "print(f\"Training completed in {pretraining_ac_run_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a512cbed-9eae-492a-b841-0b39131536a5",
   "metadata": {},
   "source": [
    "# Evaluate autoencoder reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b936d9d0-7330-4320-82ff-4e89648ce8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('pretrain_log.csv')\n",
    "fig= plt.figure(figsize=(7, 5))\n",
    "plt.plot(df['epoch'],df['train_loss'], color='b',label='Training Loss', linewidth=3.0)\n",
    "plt.plot(df['epoch'],df['val_loss'], color='darkorange',label='Validation Loss', linewidth=3.0)\n",
    "\n",
    "plt.ylabel('Loss', fontsize= 18)\n",
    "plt.xlabel('Epoch', fontsize= 18)\n",
    "plt.title('Reconstruction loss of the autoencoder', fontsize= 18)\n",
    "plt.yticks (fontsize= 18)\n",
    "plt.xticks (fontsize= 18)\n",
    "\n",
    "plt.legend(loc= 1, frameon= False, fontsize= 18)\n",
    "plt.tight_layout()\n",
    "plt.show ()\n",
    "fig.savefig ( 'ReconstructionLoss.png', dpi= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec57e47-07ea-4e45-81e7-d30efd3bc952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state_dict\n",
    "torch.save(model.state_dict(), 'autoencoder-model.pth')\n",
    "\n",
    "# Re-import or re-define the Autoencoder class\n",
    "autoencoder = Autoencoder()  # Instantiate the model\n",
    "autoencoder.load_state_dict(torch.load('autoencoder-model.pth'))  # Load the state_dict\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder.to(device)\n",
    "\n",
    "# Set the model to evaluation mode if you're testing or inferring\n",
    "autoencoder.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1caeb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.tensor(data_file)  # Convert to PyTorch tensor\n",
    "data_tensor = data_tensor.unsqueeze(1)  # Adds a dimension at position 1\n",
    "\n",
    "data_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e508f0-1ad1-4309-a41f-5739b8f6fa7d",
   "metadata": {},
   "source": [
    "### Autoencoder input-output visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e01374-a800-49ce-9213-5e3dded6f220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoded_imgs, _ = autoencoder(data_tensor.float().to(device))\n",
    "\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "s = 300\n",
    "e= 305\n",
    "n = e-s\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(s,e):\n",
    "    \n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i-s + 1)\n",
    "    librosa.display.specshow(data_file[i,:,:], alpha=None, cmap='hot', antialiased=True)\n",
    "    plt.colorbar ()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i-s + 1 + n)\n",
    "    librosa.display.specshow(decoded_imgs.cpu().detach().numpy()[i,0, :,:], alpha=None, cmap='hot', antialiased=True)\n",
    "    plt.colorbar ()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ec1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f604a8d6-4a34-43c7-81e3-ef55d39ba8a9",
   "metadata": {},
   "source": [
    "## Kmeans clustering based on extracted features from the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64671bcb-0ff8-414c-aacf-fa7ce2e18424",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs, encoded_imgs = autoencoder(data_tensor.float().to(device))\n",
    "\n",
    "kmeans = KMeans(n_clusters=1, random_state=46, n_init=20).fit(encoded_imgs.cpu().detach().numpy())\n",
    "y = kmeans.predict(encoded_imgs.cpu().detach().numpy())\n",
    "\n",
    "def plotter(S, y):\n",
    "    '''\n",
    "    function to visualize the outputs of t-SNE\n",
    "    '''\n",
    "    \n",
    "    lw = 2\n",
    "    # create a scatter plot.\n",
    "    f = plt.figure(figsize=(22, 10))\n",
    "    ax = f.add_subplot(111)\n",
    "    plt.scatter(S[y == 0, 0], S[y == 0, 1],color='navy', alpha=.5, lw=lw, s=100)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight') \n",
    "    plt.show()\n",
    "    f.savefig ('Tnse-km-n1', dpi= 100,bbox_inches = \"tight\")\n",
    "\n",
    "    return f, ax\n",
    "\n",
    "enc = encoded_imgs.cpu().detach().numpy()\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "redu = TSNE(random_state=123).fit_transform(enc)\n",
    "plotter(redu, y) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bc9c74",
   "metadata": {},
   "source": [
    "TODO: this plot doesn't look like the paper's plot\n",
    "This will make the following images look different from the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfcc610-b758-430f-8159-505dcad5d000",
   "metadata": {},
   "source": [
    "# Determining optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a5528-9064-4cee-b5b9-7f246de16eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "cal = []\n",
    "K = range(2,16)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=20).fit(encoded_imgs.cpu().detach().numpy())\n",
    "    labelskm = kmeans.predict(encoded_imgs.cpu().detach().numpy())\n",
    "    cal.append(calinski_harabasz_score(encoded_imgs.cpu().detach().numpy(), labelskm))\n",
    "fig= plt.figure(figsize=(7, 5))\n",
    "plt.plot(K, cal, 'bx-')\n",
    "plt.xlabel('Number of clusters',fontsize= 18)\n",
    "plt.ylabel('Calinski-Harabasz score',fontsize= 18)\n",
    "plt.title('Calinski-Harabasz Score Elbow for K-means Clustering',fontsize= 18)\n",
    "plt.yticks (fontsize= 18)\n",
    "plt.xticks (fontsize= 18)\n",
    "plt.axvline(x = 4, color = 'black')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig ( 'Calinski score.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565b504-f0a8-4324-b24c-4a21aab6f345",
   "metadata": {},
   "source": [
    "# T-sne visualizations of seismic event clusters in feature domain after pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ec66b-cd4d-426f-9fd6-255078c97428",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoded_imgs.cpu().detach().numpy()\n",
    "kmeans = KMeans(n_clusters=4, random_state=46, n_init=20).fit(encoded_imgs.cpu().detach().numpy())\n",
    "y = kmeans.predict(enc)\n",
    "\n",
    "def plotter(S, y, target_names):\n",
    "    '''\n",
    "    function to visualize the outputs of t-SNE\n",
    "    '''\n",
    "    # choose a color palette with seaborn.\n",
    "    colors = [ 'red', 'mediumblue','darkorange','turquoise','lime', 'turquoise', 'darkorange','lawngreen', 'red', 'saddlebrown']\n",
    "    \n",
    "    lw = 2\n",
    "    # create a scatter plot.\n",
    "    f = plt.figure(figsize=(22, 10))\n",
    "    ax = f.add_subplot(111)\n",
    "    for color, i, target_name in zip(colors, [3,0, 1,2], target_names):\n",
    "        plt.scatter(S[y == i, 0], S[y == i, 1], color=color, alpha=.5, lw=lw, s=100, label=target_name)\n",
    "    plt.legend(loc='lower left', shadow=False, scatterpoints=1, prop={'size': 26})\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight') \n",
    "    plt.show()\n",
    "    f.savefig ('Tnse-km-n4', dpi= 100,bbox_inches = \"tight\")\n",
    "\n",
    "    return f, ax\n",
    "\n",
    "enc = encoded_imgs.cpu().detach().numpy()\n",
    "from sklearn.manifold import TSNE\n",
    "redu = TSNE(random_state=123).fit_transform(enc)\n",
    "target_names = ['cluster 1', 'cluster 2', 'cluster 3', 'cluster 4']\n",
    "plotter(redu, y, target_names) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc368d8-35b2-4ce4-bd45-66a0ac5c3661",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d69729d-d928-432e-b3df-a094b7d185af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3f4cf-8b29-4ab4-bdaa-e32250578df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.tensor(data_file)  # Convert to PyTorch tensor\n",
    "data_tensor = data_tensor.unsqueeze(1).to(device)  # Adds a dimension at position 1\n",
    "\n",
    "\n",
    "# Assuming autoencoder and data_file are already defined\n",
    "autoencoder.eval()  # Set to evaluation mode\n",
    "\n",
    "# Define a dictionary to store layer outputs\n",
    "layer_outputs = {}\n",
    "\n",
    "# Function to create hooks for capturing the outputs\n",
    "def get_layer_output_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        layer_outputs[name] = output\n",
    "    return hook\n",
    "\n",
    "# Register hooks on each layer you want to capture (starting from layer 1 as in your example)\n",
    "for idx, layer in enumerate(list(autoencoder.children())[1:], start=1):\n",
    "    layer.register_forward_hook(get_layer_output_hook(f'layer_{idx}'))\n",
    "\n",
    "# Run a forward pass with `data_file` to capture the outputs\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    data = torch.tensor(data_tensor, dtype=torch.float32)  # Convert data_file to tensor\n",
    "    autoencoder(data)  # Run data through the model to capture intermediate outputs\n",
    "\n",
    "# Display the outputs\n",
    "for name, output in layer_outputs.items():\n",
    "    print(f\"{name} output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f5638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e41ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fb6d620-b779-4741-acf2-d6846c24949c",
   "metadata": {},
   "source": [
    "# Integrating clustering layer into autoencoder bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1fc455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 46\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Initialize Xavier initializer (same as GlorotUniform in TensorFlow)\n",
    "initializer = nn.init.xavier_uniform_\n",
    "n_clusters = 4 # Set desired number of clusters\n",
    "\n",
    "\n",
    "# Define the clustering layer in PyTorch\n",
    "class ClusteringLayer(nn.Module):\n",
    "    def __init__(self, n_clusters, input_dim, alpha=1.0):\n",
    "        super(ClusteringLayer, self).__init__()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Initialize the cluster centers\n",
    "        self.clusters = nn.Parameter(torch.Tensor(n_clusters, input_dim))\n",
    "        # Define cluster centers as a trainable parameter\n",
    "        \n",
    "        initializer(self.clusters)  # Xavier initialization\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        print(inputs.shape)\n",
    "        self.clusters = self.clusters.to(inputs.device)\n",
    "        # Compute the soft assignment q\n",
    "        q = 1.0 / (1.0 + (torch.sum((inputs.unsqueeze(1) - self.clusters) ** 2, dim=2) / self.alpha))\n",
    "        q = q ** ((self.alpha + 1.0) / 2.0)\n",
    "        q = q / torch.sum(q, dim=1, keepdim=True)  # Normalize to get probabilities per cluster\n",
    "        return q\n",
    "\n",
    "# Define the full model with the clustering layer on top of the autoencoder\n",
    "class ClusteredAutoencoder(nn.Module):\n",
    "    def __init__(self, autoencoder, n_clusters):\n",
    "        super(ClusteredAutoencoder, self).__init__()\n",
    "        self.autoencoder = autoencoder\n",
    "        # Clustering layer connected to the output of the encoder\n",
    "        self.clustering_layer = ClusteringLayer(n_clusters=n_clusters, input_dim=24)  # Match to `encoded` dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        decoded, encoded = self.autoencoder(x)\n",
    "        q = self.clustering_layer(encoded)  # Cluster assignments\n",
    "        return decoded, q  # Return both decoded output and cluster assignments\n",
    "\n",
    "# Instantiate the models\n",
    "n_clusters = 4  # Set desired number of clusters\n",
    "model = ClusteredAutoencoder(autoencoder, n_clusters=n_clusters)\n",
    "\n",
    "# Define the KL-divergence loss function\n",
    "def kld_loss(q, p):\n",
    "    return F.kl_div(q.log(), p, reduction='batchmean')\n",
    "\n",
    "# Example optimizer setup\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training loop placeholder\n",
    "def train(model, data_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, _ in data_loader:\n",
    "            decoded, q = model(inputs)\n",
    "            # Target distribution p, here using a sample softmax normalization of q as a placeholder\n",
    "            p = (q ** 2) / torch.sum(q, dim=0)\n",
    "            p = p / torch.sum(p, dim=1, keepdim=True)\n",
    "            \n",
    "            # Calculate the losses\n",
    "            reconstruction_loss = F.mse_loss(decoded, inputs)\n",
    "            clustering_loss = kld_loss(q, p)\n",
    "            total_loss = 0.1 * clustering_loss + reconstruction_loss\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.4f}')\n",
    "\n",
    "# Visualize the model (can be printed or logged)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Ensure that the input data is on the same device (for example, random input)\n",
    "dummy_input = torch.randn(1, 1, 96, 128).to(device)  # Simulate a batch of size 1\n",
    "\n",
    "# Now use torchinfo for the summary\n",
    "summary(model, input_size=(1, 1, 96, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68c845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3549ee2-a05b-4c8e-8b3f-ef5de83470b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "\n",
    "\n",
    "# Assuming autoencoder and data_file are already defined\n",
    "data_tensor = torch.tensor(data_file)  # Convert to PyTorch tensor\n",
    "data_tensor = data_tensor.unsqueeze(1).to(device)  # Adds a dimension at position 1\n",
    "\n",
    "autoencoder.eval()  # Set to evaluation mode\n",
    "\n",
    "# Define a dictionary to store layer outputs\n",
    "layer_outputs = {}\n",
    "\n",
    "# Function to create hooks for capturing the outputs\n",
    "def get_layer_output_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        layer_outputs[name] = output\n",
    "    return hook\n",
    "\n",
    "# Register hooks on each layer you want to capture (starting from layer 1 as in your example)\n",
    "for idx, layer in enumerate(list(autoencoder.children())[1:], start=1):\n",
    "    layer.register_forward_hook(get_layer_output_hook(f'layer_{idx}'))\n",
    "\n",
    "# Run a forward pass with `data_file` to capture the outputs\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    data = data_tensor.float()  # Ensure data is in float32 format\n",
    "    autoencoder(data)  # Run data through the model to capture intermediate outputs\n",
    "\n",
    "# Display the outputs\n",
    "for name, output in layer_outputs.items():\n",
    "    print(f\"{name} output shape: {output.shape}\")\n",
    "    \n",
    "    \n",
    "\n",
    "n_clusters = 4  # Define the number of clusters\n",
    "\n",
    "# Step 1: Perform KMeans on the encoded output to initialize cluster centers\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=46, n_init=20)\n",
    "encoded_output = layer_outputs['layer_5'].cpu().detach().numpy()  # Assuming this is the encoded output\n",
    "y_pred = kmeans.fit_predict(encoded_output)\n",
    "y_pred_last = np.copy(y_pred)\n",
    "\n",
    "# Step 2: Set the KMeans cluster centers as initial weights in the clustering layer\n",
    "cluster_centers = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32, device = device)\n",
    "\n",
    "# Assuming model.clustering_layer is the clustering layer in the PyTorch model\n",
    "model.clustering_layer.clusters.data = cluster_centers  # Directly assign cluster centers\n",
    "\n",
    "# Verify the initialization\n",
    "print(\"Number of clusters in KMeans model:\", kmeans.n_clusters)\n",
    "print(\"Number of clusters in clustering layer:\", model.clustering_layer.n_clusters)\n",
    "print(\"Assigned KMeans cluster centers as initial weights in the clustering layer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41359e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f34c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs  = model(data_tensor.float().to(device))[0]\n",
    "\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "s = 300\n",
    "e= 305\n",
    "n = e-s\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(s,e):\n",
    "    \n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i-s + 1)\n",
    "    librosa.display.specshow(data_file[i,:,:], alpha=None, cmap='hot', antialiased=True)\n",
    "    plt.colorbar ()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i-s + 1 + n)\n",
    "    librosa.display.specshow(decoded_imgs.cpu().detach().numpy()[i,0, :,:], alpha=None, cmap='hot', antialiased=True)\n",
    "    plt.colorbar ()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb465a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "534dbf3a-6ee3-4e02-98d9-c8733db0497e",
   "metadata": {},
   "source": [
    "# Finetuning pre-trained model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Parameters for fine-tuning\n",
    "batch_size = 32\n",
    "tol = 0.0001  # tolerance threshold for early stopping\n",
    "loss_value = 0\n",
    "index = 0\n",
    "maxiter = 300\n",
    "update_interval = 200\n",
    "index_array = np.arange(data_file.shape[0])\n",
    "y_pred_last = np.zeros(data_file.shape[0])\n",
    "\n",
    "data_tensor = torch.tensor(data_file, dtype=torch.float32)\n",
    "data_tensor = data_tensor.unsqueeze(1).to(device)  # Adds a dimension at position 1\n",
    "\n",
    "# Define the target distribution function\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / np.sum(q, axis=0)\n",
    "    return (weight.T / np.sum(weight, axis=1)).T\n",
    "\n",
    "# Fine-tuning loop\n",
    "for ite in range(maxiter):\n",
    "    print(f\"Iteration: {ite}\")\n",
    "    \n",
    "    # Update clustering target distribution periodically\n",
    "    if ite % update_interval == 0:\n",
    "        print(\"Updating target distribution\")\n",
    "        \n",
    "        # Forward pass to get cluster assignments\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            \n",
    "            _, q = model(data_tensor)\n",
    "       \n",
    "            q = q.cpu().numpy()  # Convert to numpy for further processing\n",
    "        \n",
    "        # Update the target distribution `p`\n",
    "        p = target_distribution(q)\n",
    "        y_pred = q.argmax(1)  # Get cluster assignments\n",
    "        \n",
    "        # Print loss and clustering information\n",
    "        print(f\"Iter {ite}: ; loss={round(loss_value, 5)}\")\n",
    "\n",
    "        # Check convergence criterion\n",
    "        delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        \n",
    "        if ite > 0 and delta_label < tol:\n",
    "            print(f\"Convergence achieved: delta_label {delta_label} < tol {tol}\")\n",
    "            break\n",
    "    \n",
    "    # Mini-batch training\n",
    "    start_idx = index * batch_size\n",
    "    end_idx = min((index + 1) * batch_size, data_file.shape[0])\n",
    "    idx = index_array[start_idx:end_idx]\n",
    "    \n",
    "    batch_data = data_tensor[idx]\n",
    "    batch_p = torch.tensor(p[idx], dtype=torch.float32, device = device)\n",
    "\n",
    "    # Zero gradients, forward pass, compute loss, backward pass, and update\n",
    "    optimizer.zero_grad()\n",
    "    decoded, q_batch = model(batch_data)\n",
    "    clustering_loss = F.kl_div(q_batch.log(), batch_p, reduction=\"batchmean\")\n",
    "    reconstruction_loss = F.mse_loss(decoded, batch_data)\n",
    "    total_loss = clustering_loss * 0.1 + reconstruction_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update loss and index for next iteration\n",
    "    loss_value = total_loss.item()\n",
    "    index = (index + 1) % (data_file.shape[0] // batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68f634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9705825-856b-4645-a5aa-414155ef00a8",
   "metadata": {},
   "source": [
    "# T-sne visualizations of seismic event clusters in feature domain after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5f876-8994-4086-9074-ae0780173492",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.eval()  # Set to evaluation mode\n",
    "\n",
    "# Define a dictionary to store layer outputs\n",
    "layer_outputs = {}\n",
    "\n",
    "# Function to create hooks for capturing the outputs\n",
    "def get_layer_output_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        layer_outputs[name] = output\n",
    "    return hook\n",
    "\n",
    "# Register hooks on each layer you want to capture (starting from layer 1 as in your example)\n",
    "for idx, layer in enumerate(list(autoencoder.children())[1:], start=1):\n",
    "    layer.register_forward_hook(get_layer_output_hook(f'layer_{idx}'))\n",
    "\n",
    "# Run a forward pass with `data_file` to capture the outputs\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    data = data_tensor.float()  # Ensure data is in float32 format\n",
    "    autoencoder(data)  # Run data through the model to capture intermediate outputs\n",
    "\n",
    "y=y_pred\n",
    "\n",
    "def plotter(S, y, target_names):\n",
    "    '''\n",
    "    function to visualize the outputs of t-SNE\n",
    "    '''\n",
    "    # choose a color palette with seaborn.\n",
    "    colors = ['red', 'mediumblue','darkorange','turquoise', 'lawngreen', 'red', 'saddlebrown']\n",
    "    \n",
    "    lw = 2\n",
    "    # create a scatter plot.\n",
    "    f = plt.figure(figsize=(22, 10))\n",
    "    ax = f.add_subplot(111)\n",
    "    for color, i, target_name in zip(colors, [3,0, 1,2], target_names):\n",
    "        plt.scatter(S[y == i, 0], S[y == i, 1], color=color, alpha=0.5, lw=lw, s=100, label=target_name)\n",
    "    plt.legend(loc='lower left', shadow=False, scatterpoints=1, prop={'size': 26})\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight') \n",
    "    plt.show()\n",
    "    f.savefig ('Tsne-km-n4-ft.png', dpi= 100,bbox_inches = \"tight\")\n",
    "    \n",
    "    return f, ax\n",
    "\n",
    "enc = layer_outputs['layer_5'].cpu().detach().numpy()\n",
    "from sklearn.manifold import TSNE\n",
    "redu = TSNE(random_state=123).fit_transform(enc)\n",
    "target_names = [ 'Earthquakes (EQ)','Continuous tremors 1 (CT1)', 'Episodic tremors (ET)', 'Continuous tremors 2 (CT2)' ]\n",
    "plotter(redu, y, target_names) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a10236-cc2f-48df-8ac5-7482e19f9412",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the labels\n",
    "np.savetxt('km-n4-ft.txt', y, fmt='%i', delimiter=',')\n",
    "\n",
    "# Change the order of the cluster numbers (just for a nice representation)\n",
    "\n",
    "with open('km-n4-ft.txt', 'r') as file :\n",
    "  filedata = file.read()\n",
    "\n",
    "# Replace the target string\n",
    "filedata = filedata.replace('3', 'data_file')\n",
    "filedata = filedata.replace('2', '3')\n",
    "filedata = filedata.replace('1', '2')\n",
    "filedata = filedata.replace('0', '1')\n",
    "filedata = filedata.replace('data_file', '0')\n",
    "\n",
    "# Re-write the output\n",
    "with open('km-n4-ft.txt', 'w') as file:\n",
    "  file.write(filedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb231f-9bae-4f09-8adf-906789cec64d",
   "metadata": {},
   "source": [
    "### Visualizing cluster changes across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a1b419-d268-47f7-a850-037dc1bd6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "y= np.loadtxt ('km-n4-ft.txt')\n",
    "fig= plt.figure(figsize=(18, 4))\n",
    "ax = fig.add_subplot()\n",
    "x1=list(range(0,2390))\n",
    "colors = ['red', 'mediumblue','darkorange','turquoise']\n",
    "cmap_name = 'my_list'\n",
    "cmap = LinearSegmentedColormap.from_list(cmap_name, colors)\n",
    "ax.scatter (x1, y, c=y, cmap=cmap, s= 30, alpha=0.2)\n",
    "ax.set_yticks ([0, 1, 2,3])\n",
    "ax.set_yticklabels (['EQ','CT1', 'ET','CT2'], fontsize=18)\n",
    "plt.ylabel('Clusters', fontsize= 18)\n",
    "\n",
    "ax.set_xticks ([0,175,417,708,996,1117,1425,1782,2142,2390])\n",
    "ax.set_xticklabels (['12 March','19 March','30 March','15 April','27 April','2 May','15 May','30 May','14 June','24 June'], fontsize=16)\n",
    "ax.axvline (x=174, linewidth=4, color='black')\n",
    "ax.axvline (x=996, linewidth=4, color='black')\n",
    "ax.axvline (x=2127, linewidth=4, color='black')\n",
    "\n",
    "plt.xlim (0,2390)\n",
    "#plt.xlim (980,1020)\n",
    "plt.tight_layout()\n",
    "fig.savefig ( 'Temporal Cluster Changes.png', dpi= 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ff7eec-7915-4152-85ce-4aa2c978466f",
   "metadata": {},
   "source": [
    "# Cluster-wise autoencoder input-output visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs[43,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a56f5-eea9-451d-a61d-3b6a480110f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import savefig\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "decoded_imgs, _ = autoencoder(data_tensor.float().to(device))\n",
    "decoded_imgs = decoded_imgs.cpu().detach().numpy()\n",
    "\n",
    "fig= plt.figure(figsize=(15, 5))\n",
    "spec = gridspec.GridSpec(2, 3)\n",
    "\n",
    "ax1 = fig.add_subplot(spec[0, 0])\n",
    "\n",
    "librosa.display.specshow(data_file[43,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax1.set(ylabel=None)\n",
    "plt.ylabel('Frequency (Hz)', fontsize= 18)\n",
    "\n",
    "ax2 = fig.add_subplot(spec[0, 1])\n",
    "\n",
    "librosa.display.specshow(data_file[44,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks ([])\n",
    "ax2.set(ylabel=None)\n",
    "\n",
    "fig.suptitle ('Cluster EQ', fontsize= 22)\n",
    "\n",
    "ax3 = fig.add_subplot(spec[0, 2])\n",
    "\n",
    "librosa.display.specshow(data_file[45,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks ([])\n",
    "ax3.set(ylabel=None)\n",
    "\n",
    "ax4 = fig.add_subplot(spec[1, 0])\n",
    "\n",
    "librosa.display.specshow(decoded_imgs[43,0,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax4.set(ylabel=None)\n",
    "labelsx = [0,30,60]\n",
    "plt.xticks(np.arange(0,128, 63.999), labelsx, fontsize= 18)\n",
    "plt.ylabel('Frequency (Hz)', fontsize= 18)\n",
    "\n",
    "ax5 = fig.add_subplot(spec[1, 1])\n",
    "\n",
    "librosa.display.specshow(decoded_imgs[44,0,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks ([])\n",
    "ax5.set(ylabel=None)\n",
    "ax5.set_xlabel(\"Time (min)\", fontsize= 18)\n",
    "labelsx = [0,30,60]\n",
    "plt.xticks(np.arange(0,128, 63.999), labelsx, fontsize=18)\n",
    "\n",
    "ax6 = fig.add_subplot(spec[1, 2])\n",
    "\n",
    "librosa.display.specshow(decoded_imgs[45,0,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks ([])\n",
    "ax6.set(ylabel=None)\n",
    "labelsx = [0,30,60]\n",
    "plt.xticks(np.arange(0,128, 63.999), labelsx, fontsize= 18)\n",
    "\n",
    "plt.tight_layout()    \n",
    "plt.savefig ('In&out-EQ.png', dpi=100, bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6c865-6715-4321-b573-55e6998cba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(15, 5))\n",
    "spec = gridspec.GridSpec(2, 3)\n",
    "\n",
    "ax1 = fig.add_subplot(spec[0, 0])\n",
    "\n",
    "librosa.display.specshow(data_file[353,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax1.set(ylabel=None)\n",
    "plt.ylabel('Frequency (Hz)', fontsize= 18)\n",
    "\n",
    "ax2 = fig.add_subplot(spec[0, 1])\n",
    "\n",
    "librosa.display.specshow(data_file[354,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax2.set(ylabel=None)\n",
    "\n",
    "fig.suptitle ('Cluster CT1', fontsize= 22)\n",
    "\n",
    "ax3 = fig.add_subplot(spec[0, 2])\n",
    "\n",
    "librosa.display.specshow(data_file[355,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax3.set(ylabel=None)\n",
    "\n",
    "ax4 = fig.add_subplot(spec[1, 0])\n",
    "\n",
    "librosa.display.specshow(decoded_imgs[353,0,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax4.set(ylabel=None)\n",
    "labelsx = [0,30,60]\n",
    "plt.xticks(np.arange(0,128, 63.999), labelsx, fontsize= 18)\n",
    "plt.ylabel('Frequency (Hz)', fontsize= 18)\n",
    "\n",
    "ax5 = fig.add_subplot(spec[1, 1])\n",
    "\n",
    "librosa.display.specshow(decoded_imgs[354,0,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax5.set(ylabel=None)\n",
    "ax5.set_xlabel(\"Time (min)\", fontsize= 18)\n",
    "labelsx = [0,30,60]\n",
    "plt.xticks(np.arange(0,128, 63.999), labelsx, fontsize= 18)\n",
    "\n",
    "ax6 = fig.add_subplot(spec[1, 2])\n",
    "\n",
    "librosa.display.specshow(decoded_imgs[355,0,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax6.set(ylabel=None)\n",
    "labelsx = [0,30,60]\n",
    "plt.xticks(np.arange(0,128, 63.999), labelsx, fontsize= 18)\n",
    "\n",
    "plt.tight_layout()    \n",
    "plt.savefig ('In&out-CT1.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cfa3f0-4f37-426b-b729-9b040894cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import savefig\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "fig= plt.figure(figsize=(15, 5))\n",
    "spec = gridspec.GridSpec(2, 3)\n",
    "\n",
    "ax1 = fig.add_subplot(spec[0, 0])\n",
    "\n",
    "librosa.display.specshow(data_file[1169,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax1.set(ylabel=None)\n",
    "plt.ylabel('Frequency (Hz)', fontsize= 18)\n",
    "\n",
    "ax2 = fig.add_subplot(spec[0, 1])\n",
    "\n",
    "librosa.display.specshow(data_file[1170,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax2.set(ylabel=None)\n",
    "\n",
    "fig.suptitle ('Cluster ET', fontsize= 22)\n",
    "\n",
    "ax3 = fig.add_subplot(spec[0, 2])\n",
    "\n",
    "librosa.display.specshow(data_file[1171,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax3.set(ylabel=None)\n",
    "\n",
    "ax4 = fig.add_subplot(spec[1, 0])\n",
    "\n",
    "librosa.display.specshow(decoded_imgs[1169,0,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax4.set(ylabel=None)\n",
    "labelsx = [0,30,60]\n",
    "plt.xticks(np.arange(0,128, 63.999), labelsx, fontsize= 18)\n",
    "plt.ylabel('Frequency (Hz)', fontsize= 18)\n",
    "\n",
    "ax5 = fig.add_subplot(spec[1, 1])\n",
    "\n",
    "librosa.display.specshow(decoded_imgs[1170,0,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax5.set(ylabel=None)\n",
    "ax5.set_xlabel(\"Time (min)\", fontsize= 20)\n",
    "labelsx = [0,30,60]\n",
    "plt.xticks(np.arange(0,128, 63.999), labelsx, fontsize= 18)\n",
    "\n",
    "ax6 = fig.add_subplot(spec[1, 2])\n",
    "\n",
    "librosa.display.specshow(decoded_imgs[1171,0,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax6.set(ylabel=None)\n",
    "labelsx = [0,30,60]\n",
    "plt.xticks(np.arange(0,128, 63.999), labelsx, fontsize= 18)\n",
    "\n",
    "plt.tight_layout()    \n",
    "plt.savefig ('In&out-ET.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2455d0-5590-4b47-b51f-32033b7c71e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import savefig\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "fig= plt.figure(figsize=(15, 5))\n",
    "spec = gridspec.GridSpec(2, 3)\n",
    "\n",
    "ax1 = fig.add_subplot(spec[0, 0])\n",
    "\n",
    "librosa.display.specshow(data_file[2342,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax1.set(ylabel=None)\n",
    "plt.ylabel('Frequency (Hz)', fontsize= 18)\n",
    "\n",
    "ax2 = fig.add_subplot(spec[0, 1])\n",
    "\n",
    "librosa.display.specshow(data_file[2343,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax2.set(ylabel=None)\n",
    "\n",
    "fig.suptitle ('Cluster CT2', fontsize= 22)\n",
    "\n",
    "ax3 = fig.add_subplot(spec[0, 2])\n",
    "\n",
    "librosa.display.specshow(data_file[2344,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax3.set(ylabel=None)\n",
    "\n",
    "ax4 = fig.add_subplot(spec[1, 0])\n",
    "\n",
    "librosa.display.specshow(decoded_imgs[2342,0,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax4.set(ylabel=None)\n",
    "labelsx = [0,30,60]\n",
    "plt.xticks(np.arange(0,128, 63.999), labelsx, fontsize= 18)\n",
    "plt.ylabel('Frequency (Hz)', fontsize= 18)\n",
    "\n",
    "ax5 = fig.add_subplot(spec[1, 1])\n",
    "\n",
    "librosa.display.specshow(decoded_imgs[2343,0,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax5.set(ylabel=None)\n",
    "ax5.set_xlabel(\"Time (min)\", fontsize= 20)\n",
    "labelsx = [0,30,60]\n",
    "plt.xticks(np.arange(0,128, 63.999), labelsx, fontsize= 18)\n",
    "\n",
    "ax6 = fig.add_subplot(spec[1, 2])\n",
    "\n",
    "librosa.display.specshow(decoded_imgs[2344,0,:,:], alpha=None, cmap='hot', antialiased=True,y_axis='linear', sr= 8, vmin=-1 , vmax=10)\n",
    "cbar= plt.colorbar(pad= 0.03)\n",
    "cbar.ax.tick_params(labelsize=18, rotation=0)\n",
    "freq =[0,1.33, 2.66, 4]\n",
    "labelsy = [1,2,3, 4]\n",
    "plt.yticks (freq, labelsy, fontsize= 18)\n",
    "ax6.set(ylabel=None)\n",
    "labelsx = [0,30,60]\n",
    "plt.xticks(np.arange(0,128, 63.999), labelsx, fontsize= 18)\n",
    "\n",
    "plt.tight_layout()    \n",
    "plt.savefig ('In&out-CT2.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9243c6-82b7-42af-b81f-2f58e26490ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a089c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
